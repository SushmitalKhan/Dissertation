{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushmitalKhan/Dissertation/blob/main/infer_condition2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qg_PykDMiJsX"
      },
      "outputs": [],
      "source": [
        "This condition will not ask users what type of inference they want. It will generate inferences, and rate the uncommonness and sensitivity of the inferences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = 'read api key'\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # GPT-3.5 & GPT-4 use GPT-2 tokenizer\n",
        "\n",
        "def count_tokens(text):\n",
        "    \"\"\"Returns the number of tokens in a given text.\"\"\"\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "def truncate_list(data_list, max_tokens):\n",
        "    \"\"\"Truncates a list to fit within a token budget.\"\"\"\n",
        "    truncated_list = []\n",
        "    token_count = 0\n",
        "    for item in data_list:\n",
        "        item_tokens = count_tokens(str(item))  # Count tokens in each item\n",
        "        if token_count + item_tokens <= max_tokens:\n",
        "            truncated_list.append(item)\n",
        "            token_count += item_tokens\n",
        "        else:\n",
        "            break\n",
        "    return truncated_list\n",
        "\n",
        "def run_prompt(prompt):\n",
        "    response = openai.chat.completions.create(\n",
        "        # model=\"gpt-3.5-turbo\",  # Changed 'engine' to 'model'\n",
        "        model=\"gpt-4o-mini\",\n",
        "        # model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=1000,\n",
        "        # max_tokens=min(1000, 16385 - input_token_count)  # Ensure within token limit\n",
        "        temperature=0.7,\n",
        "        n=1, # Return only one response\n",
        "        stop=None # optional stopping sequence\n",
        "        # request_timeout = 60\n",
        "        )\n",
        "    json_response = response.choices[0].message.content.strip()\n",
        "\n",
        "    return json_response  # Convert response to Python dictionary"
      ],
      "metadata": {
        "id": "NO7r4K6ZiQ3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PFs-jGsmVHV",
        "outputId": "a57c76ed-e6fa-409f-c97f-2c23da8d2ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_json(output_filename, inference_json):\n",
        "    if not inference_json:\n",
        "        print(\"‚ö†Ô∏è No data to write. JSON file not created.\")\n",
        "        return None\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(inference_json, f, indent=4, ensure_ascii=False)\n",
        "    print(f\"‚úÖ JSON saved successfully as {output_filename}\")\n",
        "    return output_filename"
      ],
      "metadata": {
        "id": "Zk7D1yOgGB3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_inference_json(inference_data, inference_no, columns_used):\n",
        "    \"\"\"Saves the inference data to a JSON file with an automated filename.\"\"\"\n",
        "    base_dir = \"/content/drive/MyDrive/Dissertation/Study 1/Inference_data\"\n",
        "\n",
        "    # Create a descriptive filename based on inference number and columns\n",
        "    column_names = \"_\".join([col.split(\"_\")[1] for col in columns_used])\n",
        "    filename = f\"inferences_{inference_no}_{len(columns_used)}cols_{column_names}.json\"\n",
        "    output_filename = f\"{base_dir}/{filename}\"\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(inference_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ JSON saved successfully as {output_filename}\")"
      ],
      "metadata": {
        "id": "fQVZahjI1hr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "powerset2023 = pd.read_csv('/content/drive/MyDrive/Dissertation/Study 1/Powerset/powerset_by_year/allActivity_2023.csv')"
      ],
      "metadata": {
        "id": "2wo2rXSeCiPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns with less than 600 non-NaN rows\n",
        "misc_columns = powerset2023.columns[powerset2023.notna().sum() < 600]\n",
        "\n",
        "# Combine values into a new column\n",
        "powerset2023['takeout1_misc_MyActivity_Search Title'] = powerset2023[misc_columns].apply(\n",
        "    lambda row: ', '.join(row.dropna().astype(str)),\n",
        "    axis=1)\n",
        "\n",
        "# Drop the identified misc_columns\n",
        "powerset2023 = powerset2023.drop(columns=misc_columns)\n",
        "\n",
        "\n",
        "# Drop duplicate values in the specified column, keeping the first occurrence\n",
        "powerset2023 = powerset2023.drop_duplicates(subset=['takeout1_chrome_MyActivity_Search Title'], keep='first')"
      ],
      "metadata": {
        "id": "7UyVcx9OPmrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make prompt more dynamic --> ask for user input for inference and recommendation #\n",
        "\n",
        "*prompt: recommend one product based on these three inferences*\n",
        "\n",
        "make numeric instead of text --> parameterize the model --> multiply it out --> if you recommend one product based on the inferences; for example inference 1 recommendation 1, inference 1 recommendation 2 and so on"
      ],
      "metadata": {
        "id": "b9K6g48ZC-ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ask to select any four column\n",
        "print(\"\\nPlease select four datasets from the options (datasets should not be the same):\")\n",
        "for i, col in enumerate(powerset2023.columns, 1):\n",
        "    print(f\"{i}. {col}\")\n",
        "\n",
        "# Ask the user to select any 4 columns\n",
        "selected_columns = []\n",
        "df_selected = pd.DataFrame()\n",
        "while len(selected_columns) < 4:\n",
        "    try:\n",
        "        col_num = int(input(f\"\\nSelect column {len(selected_columns) + 1} by number (1-{len(powerset2023.columns)}): \"))\n",
        "        if 1 <= col_num <= len(powerset2023.columns):\n",
        "            col_name = powerset2023.columns[col_num - 1]\n",
        "            if col_name not in selected_columns:\n",
        "                selected_columns.append(col_name)\n",
        "            else:\n",
        "                print(\"You've already selected this column. Try another.\")\n",
        "        else:\n",
        "            print(\"Invalid number. Please choose a number from the list.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# Extract the selected columns\n",
        "powerSubset2023 = powerset2023[selected_columns]\n",
        "\n",
        "# Print the selected DataFrame\n",
        "print(\"\\nSelected DataFrame:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxDLyhx6PGBK",
        "outputId": "6a5e7aaf-d76c-4979-97b8-cc0dc6d176af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Please select four datasets from the options (datasets should not be the same):\n",
            "1. takeout1_imageSearch_MyActivity_Search Title\n",
            "2. takeout1_chrome_MyActivity_Search Title\n",
            "3. takeout1_maps_MyActivity_Search Title\n",
            "4. takeout1_YT_search-history_Search Title\n",
            "5. takeout1_YT_watch-history_Search Title\n",
            "6. takeout1_misc_MyActivity_Search Title\n",
            "\n",
            "Select column 1 by number (1-6): 1\n",
            "\n",
            "Select column 2 by number (1-6): 2\n",
            "\n",
            "Select column 3 by number (1-6): 4\n",
            "\n",
            "Select column 4 by number (1-6): 6\n",
            "\n",
            "Selected DataFrame:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Optionally, save to a CSV file\n",
        "save_option = input(\"\\nWould you like to save this selection to a CSV file? (yes/no): \").strip().lower()\n",
        "if save_option == \"yes\":\n",
        "    df_selected.to_csv(\"/content/drive/MyDrive/Dissertation/Study 1/Inference_data/24APRIL2025/chrome_maps_YTwatch_misc.csv\", index=False)\n",
        "    # print(\"File saved as 'selected_input_data.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VvySh6B6C1z",
        "outputId": "8dd82435-461d-42c5-c004-151d05e8da06"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Would you like to save this selection to a CSV file? (yes/no): no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ask to select how many inferences they wat\n",
        "\n",
        "# Define the options\n",
        "options = [3,6,9,12]\n",
        "\n",
        "# Display the options\n",
        "print(\"Please select a number from the options below by typing the corresponding number:\\n\")\n",
        "for i, option in enumerate(options, 1):\n",
        "    print(f\"{i}. {option}\")\n",
        "\n",
        "# Get user input\n",
        "user_input = input(\"\\nEnter your selection (1-4): \")\n",
        "\n",
        "# Validate input\n",
        "if user_input.isdigit() and 1 <= int(user_input) <= 4:\n",
        "    inference_no = options[int(user_input) - 1]\n",
        "    print(f\"\\nYou selected: {inference_no}\")\n",
        "else:\n",
        "    print(\"\\nInvalid selection. Please enter a number between 1 and 4.\")\n",
        "\n",
        "# inference_input = int(inference_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Y43axPSFXV",
        "outputId": "14a5f715-ca1e-498c-bac7-395ea457fc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please select a number from the options below by typing the corresponding number:\n",
            "\n",
            "1. 3\n",
            "2. 6\n",
            "3. 9\n",
            "4. 12\n",
            "\n",
            "Enter your selection (1-4): 1\n",
            "\n",
            "You selected: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize list to store responses\n",
        "inference_c0_json = []\n",
        "\n",
        "# Iterate through each column in power_subset1_\n",
        "for col_name, col_data in powerSubset2023[:7500].items():\n",
        "  # print(col_data.dropna().tolist())\n",
        "  # Define the strict JSON prompt\n",
        "\n",
        "  prompt = f\"\"\"User wants to know their interests based on {col_data.dropna().tolist()} in {col_name}\n",
        "  1. {inference_no} inferences about the user based on their online behavior in the form \"interested in X\".\n",
        "  2. For each interest, rate uncommonness of the interest (scale 1-10, where 1 = most uncommon and 10 = most common).\n",
        "  Uncommon refers to how distinctive digital interaction patterns. demographic characteristics\n",
        "\n",
        "  3. For each interest, rate sensitivity of interest (scale 1‚Äì10, where 1 = least sensitive and 10 = most sensitive).\n",
        "  Sensitivity refers to how likely an interest is to reveal personal, private, or stigmatized information about the user ‚Äî\n",
        "  especially information they may not want to be shared, inferred, or acted upon without their consent.\n",
        "  It includes how much harm, discomfort, or unwanted exposure could result from the disclosure of that interest\n",
        "\n",
        "  4. Recommend **EXACTLY ONE** product for *EACH* inferences from step 1\n",
        "  5. Recommend **EXACTLY ONE** product based on the **combination of all {inference_no} inferences** for **{col_name}**.\n",
        "   **STRICT JSON OUTPUT FORMAT (No extra text or explanations, just JSON):**\n",
        "   {{\n",
        "    \"columns\": {col_name},\n",
        "    \"inferences\": [\n",
        "    {', '.join([json.dumps({\n",
        "      \"inference\": f\"<output for inference {i+1}>\",\n",
        "      \"explanation_inference\": f\"<explanation for inference {i+1}>\",\n",
        "      \"commonness\": f\"<Inference 1 commonness score: {i+1}>\",\n",
        "      \"explanation_commonness\": f\"<Explanation for commonness score: {i+1}>\",\n",
        "      \"sensitivity\": f\"<Inference 1 sensitivity score: {i+1}>\",\n",
        "      \"explanation_sensitivity\": f\"<Explanation for sensitivity score: {i+1}>\",\n",
        "      \"recommendation\": f\"<recommendation for inference {i+1}>\"\n",
        "    }) for i in range(inference_no)])}\n",
        "    ],\n",
        "    \"final_product_recommendation\": {{\n",
        "      \"ONE recommendation based on ALL inferenceces\": \"<Product name and company>\",\n",
        "   }}\n",
        "  }}\"\"\"\n",
        "\n",
        "  print(f\"üîπ Sending prompt to GPT for column: {col_name}...\")\n",
        "  assistant_reply = run_prompt(prompt)\n",
        "  # print(assistant_reply)\n",
        "  inference_c0_json.append(assistant_reply)"
      ],
      "metadata": {
        "id": "5aE9sD2U41E_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088aa2f0-ada4-4491-bab8-67e05a678357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Sending prompt to GPT for column: takeout1_chrome_MyActivity_Search Title...\n",
            "üîπ Sending prompt to GPT for column: takeout1_maps_MyActivity_Search Title...\n",
            "üîπ Sending prompt to GPT for column: takeout1_YT_watch-history_Search Title...\n",
            "üîπ Sending prompt to GPT for column: takeout1_misc_MyActivity_Search Title...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_c0_json_copy = inference_c0_json.copy()"
      ],
      "metadata": {
        "id": "FX6KRBEk-dl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_json_string(json_string):\n",
        "  \"\"\"Cleans a JSON string by removing unwanted characters and standardizing spacing.\"\"\"\n",
        "\n",
        "  # cleaned = re.sub(r\"```json \", \"\", json_string)\n",
        "  # Replace extra spaces and newlines with single spaces\n",
        "  cleaned = re.sub(r\"```json|```\", \" \", json_string)\n",
        "  # Remove characters outside the basic multilingual plane\n",
        "  cleaned = re.sub(r\"[^\\u0000-\\uFFFF]\", \"\", cleaned)\n",
        "  # Try to parse the cleaned JSON; return original if invalid\n",
        "  try:\n",
        "    return json.loads(cleaned)\n",
        "  except json.JSONDecodeError:\n",
        "    print(f\"Warning: Failed to parse JSON, returning original: {json_string[:50]}...\")\n",
        "    return cleaned\n",
        "\n",
        "# Apply the function to each item in your list"
      ],
      "metadata": {
        "id": "TUR2HR-8DnM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_reply_c0 = [clean_json_string(item) for item in inference_c0_json]"
      ],
      "metadata": {
        "id": "V7D2qwRHD6kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c0_i3.json\"\n",
        "write_json(output_filename, cleaned_reply_c0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dwz-2gydNY4N",
        "outputId": "dd30c6de-0a41-42e4-895c-27b0e18be26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON saved successfully as /content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c0_i3.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c0_i3.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_c2 = []\n",
        "inference_c2_json = []\n",
        "\n",
        "index = 0  # Keeps col_name[index] constant\n",
        "col_names = list(powerSubset2023[:7500].columns)  # Convert column names to a list\n",
        "\n",
        "while index < len(col_names):  # Outer loop for the constant column\n",
        "    for next_index in range(index + 1, len(col_names)):  # Inner loop for incrementing column\n",
        "        # Fetch data for the columns\n",
        "        col_data_1 = powerSubset2023[:4500][col_names[index]]\n",
        "        col_data_2 = powerSubset2023[:4500][col_names[next_index]]\n",
        "\n",
        "        # Define the prompt\n",
        "        prompt = f\"\"\"User wants to know what {col_data_1.dropna().tolist()} in {col_names[index]} and {col_data_2.dropna().tolist()} in {col_names[next_index]} tells about them.\n",
        "        1. {inference_no} inferences about the user based on their online behavior in the form \"interested in X\".\n",
        "        2. For each interest, rate uncommonness of the interest (scale 1-10, where 1 = most uncommon and 10 = most common). Uncommon refers to how rare or atypical an interest is among people who share similar demographic characteristics\n",
        "        3. For each interest, rate sensitivity of interest (scale 1‚Äì10, where 1 = least sensitive and 10 = most sensitive). Sensitivity refers to how likely an interest is to reveal personal, private, or stigmatized information about the user ‚Äî especially information they may not want to be shared, inferred, or acted upon without their consent. It includes how much harm, discomfort, or unwanted exposure could result from the disclosure of that interest\n",
        "        4. Recommend **EXACTLY ONE** product for *EACH GROUP* identified in step 3\n",
        "        5. Recommend **EXACTLY ONE** product based on the **combination of all {inference_no} inferences** in **{col_names[index]} AND {col_names[next_index]}.**\n",
        "\n",
        "   **STRICT JSON OUTPUT FORMAT (No extra text or explanations, just JSON):**\n",
        "   ```json\n",
        "   {{\n",
        "    \"columns\": \"{col_names[index]} AND {col_names[next_index]}\",\n",
        "    \"inferences\": [\n",
        "    {', '.join([json.dumps({\n",
        "      \"inference\": f\"<output for inference {i+1}>\",\n",
        "      \"explanation_inference\": f\"<explanation for inference {i+1}>\",\n",
        "      \"commonness\": f\"<Inference 1 commonness score: >\",\n",
        "      \"explanation_commonness\": f\"<Explanation for commonness score: >\",\n",
        "      \"sensitivity\": f\"<Inference 1 sensitivity score: {i+1}>\",\n",
        "      \"explanation_sensitivity\": f\"<Explanation for sensitivity score: {i+1}>\",\n",
        "      \"recommendation\": f\"<recommendation for inference {i+1}>\"\n",
        "    }) for i in range(inference_no)])}\n",
        "    ],\n",
        "    \"final_product_recommendation\": {{\n",
        "      \"ONE recommendation based on ALL inferenceces\": \"<Product name and company>\",\n",
        "    }}\n",
        "   }}\n",
        "   ```\n",
        "  }}\n",
        "\n",
        "    **Strictly return a valid JSON object in this format. DO NOT include any other text or explanations.**\"\"\"\n",
        "\n",
        "        # Run the prompt and get the response\n",
        "        print(f\"üîπ Sending prompt to GPT for column: **{col_names[index]} AND {col_names[next_index]}.**...\")\n",
        "        assistant_reply = run_prompt(prompt)\n",
        "        # print(assistant_reply)\n",
        "        inference_c2_json.append(assistant_reply)\n",
        "\n",
        "        # if assistant_reply:\n",
        "        #     # Remove unwanted code block markers\n",
        "        #     cleaned_json = re.sub(r\"```json\\s*|\\s*```\", \"\", assistant_reply).strip()\n",
        "\n",
        "        #     try:\n",
        "        #         # Parse JSON\n",
        "        #         parsed_json = json.loads(cleaned_json)\n",
        "        #         inference_c2_json.append(parsed_json)  # Store parsed data\n",
        "        #     except json.JSONDecodeError:\n",
        "        #         print(f\"‚ö†Ô∏è JSON Decoding Error. Skipping entry:\\n{cleaned_json[:100]}...\")  # Preview problematic JSON\n",
        "\n",
        "\n",
        "    # Move to the next constant column\n",
        "    index += 1"
      ],
      "metadata": {
        "id": "2lc5H2weCWBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d6efde-03e8-4b09-d03a-1ab14ac30374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Sending prompt to GPT for column: **takeout1_chrome_MyActivity_Search Title AND takeout1_maps_MyActivity_Search Title.**...\n",
            "üîπ Sending prompt to GPT for column: **takeout1_chrome_MyActivity_Search Title AND takeout1_YT_watch-history_Search Title.**...\n",
            "üîπ Sending prompt to GPT for column: **takeout1_chrome_MyActivity_Search Title AND takeout1_misc_MyActivity_Search Title.**...\n",
            "üîπ Sending prompt to GPT for column: **takeout1_maps_MyActivity_Search Title AND takeout1_YT_watch-history_Search Title.**...\n",
            "üîπ Sending prompt to GPT for column: **takeout1_maps_MyActivity_Search Title AND takeout1_misc_MyActivity_Search Title.**...\n",
            "üîπ Sending prompt to GPT for column: **takeout1_YT_watch-history_Search Title AND takeout1_misc_MyActivity_Search Title.**...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_reply_c2 = [clean_json_string(item) for item in inference_c2_json]"
      ],
      "metadata": {
        "id": "yHkOl0MRQ-St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c2_i3.json\"\n",
        "write_json(output_filename, cleaned_reply_c2)"
      ],
      "metadata": {
        "id": "b9gQKksAK_iv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ae44e303-e181-41cc-daa2-62dedd6b31d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON saved successfully as /content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c2_i3.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c2_i3.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_c3 = []\n",
        "inference_c3_json = []\n",
        "\n",
        "index = 0  # Keeps col_name[index] constant\n",
        "col_names = list(powerSubset2023[:7500].columns)  # Convert column names to a list\n",
        "\n",
        "# Outer loop keeps col_name[index] constant\n",
        "while index < len(col_names):\n",
        "    for next_index in range(index + 1, len(col_names) - 1):  # Iterate for combinations\n",
        "        # Fetch data for the selected columns\n",
        "        col_data_1 = powerSubset2023[:2500][col_names[index]]\n",
        "        col_data_2 = powerSubset2023[:2500][col_names[next_index]]\n",
        "        col_data_3 = powerSubset2023[:2500][col_names[next_index + 1]]\n",
        "\n",
        "        # Define the prompt\n",
        "        prompt = f\"\"\"User wants to know what {col_data_1.dropna().tolist()} in {col_names[index]} AND {col_data_2.dropna().tolist()} in {col_names[next_index]} AND {col_data_3.dropna().tolist()} in {col_names[next_index+1]} tells about them.\n",
        "        1. {inference_no} inferences about the user based on their online behavior in the form \"interested in X\".\n",
        "        2. For each interest, rate uncommonness of the interest (scale 1-10, where 1 = most uncommon and 10 = most common). Uncommon refers to how rare or atypical an interest is among people who share similar demographic characteristics\n",
        "        3. For each interest, rate sensitivity of interest (scale 1‚Äì10, where 1 = least sensitive and 10 = most sensitive). Sensitivity refers to how likely an interest is to reveal personal, private, or stigmatized information about the user ‚Äî especially information they may not want to be shared, inferred, or acted upon without their consent. It includes how much harm, discomfort, or unwanted exposure could result from the disclosure of that interest\\\n",
        "        4. Recommend **EXACTLY ONE** product for *EACH GROUP* identified in step 2\n",
        "        5. Recommend **EXACTLY ONE** product based on the **combination of all {inference_no} inferences** for **{col_names[index]} AND {col_names[next_index]} AND {col_names[next_index+1]}**.\n",
        "\n",
        "   **STRICT JSON OUTPUT FORMAT (No extra text or explanations, just JSON):**\n",
        "   ```json\n",
        "   {{\n",
        "    \"columns\": {col_names[index]} AND {col_names[next_index]} AND {col_names[next_index+1]},\n",
        "    \"inferences\": [\n",
        "    {', '.join([json.dumps({\n",
        "      \"inference\": f\"<output for inference {i+1}>\",\n",
        "      \"explanation_inference\": f\"<explanation for inference {i+1}>\",\n",
        "      \"commonness\": f\"<Inference 1 commonness score: >\",\n",
        "      \"explanation_commonness\": f\"<Explanation for commonness score: >\",\n",
        "      \"sensitivity\": f\"<Inference 1 sensitivity score: {i+1}>\",\n",
        "      \"explanation_sensitivity\": f\"<Explanation for sensitivity score: {i+1}>\",\n",
        "      \"recommendation\": f\"<recommendation for inference {i+1}>\"\n",
        "    }) for i in range(inference_no)])}\n",
        "    ],\n",
        "    \"final_product_recommendation\": {{\n",
        "      \"ONE recommendation based on ALL inferenceces\": \"<Product name and company>\",\n",
        "    }}\n",
        "   }}\n",
        "   ```\n",
        "  }}\n",
        "\n",
        "    **Strictly return a valid JSON object in this format. DO NOT include any other text or explanations.**\"\"\"\n",
        "\n",
        "        print(f\"üîπ Sending prompt to GPT for column: {col_names[index]} AND {col_names[next_index]} AND {col_names[next_index+1]}...\")\n",
        "        assistant_reply = run_prompt(prompt)\n",
        "        # print(assistant_reply)\n",
        "\n",
        "        if assistant_reply:\n",
        "            # Remove unwanted code block markers\n",
        "            cleaned_json = re.sub(r\"```json\\s*|\\s*```\", \"\", assistant_reply).strip()\n",
        "\n",
        "            try:\n",
        "                # Parse JSON\n",
        "                parsed_json = json.loads(cleaned_json)\n",
        "                inference_c3_json.append(parsed_json)  # Store parsed data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"‚ö†Ô∏è JSON Decoding Error. Skipping entry:\\n{cleaned_json[:100]}...\")  # Preview problematic JSON\n",
        "\n",
        "    index += 1"
      ],
      "metadata": {
        "id": "QO5H_B5OreL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299ea358-aed7-4bf3-8e3e-a6fe9fe54eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Sending prompt to GPT for column: takeout1_chrome_MyActivity_Search Title AND takeout1_maps_MyActivity_Search Title AND takeout1_YT_watch-history_Search Title...\n",
            "üîπ Sending prompt to GPT for column: takeout1_chrome_MyActivity_Search Title AND takeout1_YT_watch-history_Search Title AND takeout1_misc_MyActivity_Search Title...\n",
            "üîπ Sending prompt to GPT for column: takeout1_maps_MyActivity_Search Title AND takeout1_YT_watch-history_Search Title AND takeout1_misc_MyActivity_Search Title...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c3_i3.json\"\n",
        "write_json(output_filename, inference_c3_json)"
      ],
      "metadata": {
        "id": "H3vKwRvLY3lI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "e5dd798b-07ef-49fc-b07d-5b2f67e84772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON saved successfully as /content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c6_i3.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c6_i3.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_c3_json"
      ],
      "metadata": {
        "id": "5aQ2bUQuHdUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_c4 = []\n",
        "inference_c4_json = []\n",
        "\n",
        "index = 0  # Keeps col_name[index] constant\n",
        "col_names = list(powerSubset2023[:7500].columns)  # Convert column names to a list\n",
        "\n",
        "# Outer loop keeps col_name[index] constant\n",
        "while index < len(col_names):\n",
        "    for next_index in range(index + 1, len(col_names) - 2):  # Iterate for combinations\n",
        "        # Fetch data for the selected columns\n",
        "        col_data_1 = powerSubset2023[:2500][col_names[index]]\n",
        "        col_data_2 = powerSubset2023[:2500][col_names[next_index]]\n",
        "        col_data_3 = powerSubset2023[:2500][col_names[next_index + 1]]\n",
        "        col_data_4 = powerSubset2023[:2500][col_names[next_index + 2]]\n",
        "\n",
        "        # Define the prompt\n",
        "        prompt = f\"\"\"User wants to know what {col_data_1.dropna().tolist()} in {col_names[index]} AND {col_data_2.dropna().tolist()} in {col_names[next_index]} AND {col_data_3.dropna().tolist()} in {col_names[next_index+1]} AND {col_data_4.dropna().tolist()} in {col_names[next_index+2]} tells about them.\n",
        "        1. {inference_no} inferences about the user based on their online behavior in the form \"interested in X\".\n",
        "        2. For each interest, rate uncommonness of the interest (scale 1-10, where 1 = most uncommon and 10 = most common). Uncommon refers to how rare or atypical an interest is among people who share similar demographic characteristics\n",
        "        3. For each interest, rate sensitivity of interest (scale 1‚Äì10, where 1 = least sensitive and 10 = most sensitive). Sensitivity refers to how likely an interest is to reveal personal, private, or stigmatized information about the user ‚Äî especially information they may not want to be shared, inferred, or acted upon without their consent. It includes how much harm, discomfort, or unwanted exposure could result from the disclosure of that interest\\\n",
        "        4. Recommend **EXACTLY ONE** product for *EACH GROUP* identified in step 2\n",
        "        5. Recommend **EXACTLY ONE** product based on the **combination of all {inference_no} inferences** for **{col_names[index]} AND {col_names[next_index]} AND {col_names[next_index+1]}**.\n",
        "\n",
        "   **STRICT JSON OUTPUT FORMAT (No extra text or explanations, just JSON):**\n",
        "   ```json\n",
        "   {{\n",
        "    \"columns\": {col_names[index]} AND {col_names[next_index]} AND {col_names[next_index+1]} AND {col_names[next_index+2]},\n",
        "    \"inferences\": [\n",
        "    {', '.join([json.dumps({\n",
        "      \"inference\": f\"<output for inference {i+1}>\",\n",
        "      \"explanation_inference\": f\"<explanation for inference {i+1}>\",\n",
        "      \"commonness\": f\"<Inference 1 commonness score: >\",\n",
        "      \"explanation_commonness\": f\"<Explanation for commonness score: >\",\n",
        "      \"sensitivity\": f\"<Inference 1 sensitivity score: {i+1}>\",\n",
        "      \"explanation_sensitivity\": f\"<Explanation for sensitivity score: {i+1}>\",\n",
        "      \"recommendation\": f\"<recommendation for inference {i+1}>\"\n",
        "    }) for i in range(inference_no)])}\n",
        "    ],\n",
        "    \"final_product_recommendation\": {{\n",
        "      \"ONE recommendation based on ALL inferenceces\": \"<Product name and company>\",\n",
        "    }}\n",
        "   }}\n",
        "   ```\n",
        "  }}\n",
        "\n",
        "    **Strictly return a valid JSON object in this format. DO NOT include any other text or explanations.**\"\"\"\n",
        "\n",
        "        print(f\"üîπ Sending prompt to GPT for column: {col_names[index]} AND {col_names[next_index]} AND {col_names[next_index+1]}...\")\n",
        "        assistant_reply = run_prompt(prompt)\n",
        "        print(assistant_reply)\n",
        "        inference_c4_json.append(assistant_reply)\n",
        "\n",
        "    index += 1"
      ],
      "metadata": {
        "id": "NRVhmMUR36jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_reply_c4 = [clean_json_string(item) for item in inference_c4_json]"
      ],
      "metadata": {
        "id": "__N6gBVE5_5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_reply_c4"
      ],
      "metadata": {
        "id": "GnHdPVTr89Vd",
        "outputId": "742be61d-5609-4579-d3fb-e8456bef17d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'columns': 'takeout1_chrome_MyActivity_Search Title AND takeout1_maps_MyActivity_Search Title AND takeout1_YT_watch-history_Search Title AND takeout1_misc_MyActivity_Search Title',\n",
              "  'inferences': [{'inference': 'interested in digital privacy and data protection',\n",
              "    'explanation_inference': 'The user frequently searches for topics related to data privacy, personalized ads, and research on children‚Äôs privacy.',\n",
              "    'commonness': '6',\n",
              "    'explanation_commonness': 'Interest in digital privacy is relatively common, especially among individuals engaged in technology or education sectors.',\n",
              "    'sensitivity': '8',\n",
              "    'explanation_sensitivity': 'This interest could reveal personal beliefs about privacy and security which may be sensitive in discussions.',\n",
              "    'recommendation': 'Privacy Tools | Harvard University Privacy Tools Project'},\n",
              "   {'inference': 'interested in travel and local attractions',\n",
              "    'explanation_inference': 'The user has viewed various locations in South Carolina, including Myrtle Beach and local attractions.',\n",
              "    'commonness': '7',\n",
              "    'explanation_commonness': 'Interest in travel and local attractions is quite common among people, especially those living in or visiting tourist areas.',\n",
              "    'sensitivity': '3',\n",
              "    'explanation_sensitivity': 'This interest is generally low sensitivity, as it relates to leisure and does not reveal private information.',\n",
              "    'recommendation': 'Travel Guide | Lonely Planet'},\n",
              "   {'inference': 'interested in educational content and resources',\n",
              "    'explanation_inference': 'The searches include educational materials, teaching resources, and academic conferences.',\n",
              "    'commonness': '9',\n",
              "    'explanation_commonness': 'Interest in educational content is very common, particularly among students and educators.',\n",
              "    'sensitivity': '2',\n",
              "    'explanation_sensitivity': 'This interest is low sensitivity, as it pertains to professional development and academic pursuits.',\n",
              "    'recommendation': 'Teaching Resources | Teachers Pay Teachers'}],\n",
              "  'final_product_recommendation': {'ONE recommendation based on ALL inferences': 'Comprehensive Privacy Guide | Data & Society'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c4_i3.json\"\n",
        "write_json(output_filename, cleaned_reply_c4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "8-mZ9opL5I_P",
        "outputId": "8fc1f3ca-9701-477c-f5ec-43a52e8a91e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON saved successfully as /content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c4_i3.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Dissertation/Study 1/Inference_data/01MAY2025/chrome_map_ytWatch_misc_data_c4_i3.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "\n",
        "unCommon_Inferences = []\n",
        "\n",
        "for col_name, col_data in power_subset1_.items():\n",
        "  if index <= len(power_subset1_.columns):\n",
        "    prompt = f\"\"\"User wants the model to analyze {col_data} in {col_name[index]}. Based on this data:\n",
        "    1. Provide three unique inferences about this user based on the user‚Äôs distinctive digital interaction patterns. Make sure that these inferences reflect predicted characteristics of the user that are HIGHLY UNCOMMON in society.\n",
        "    2. Recommend one product or solution for each inference.\n",
        "    3. Explain briefly how you arrived at each inference\n",
        "    4. Explain briefly how you arrived at each recommendation\n",
        "    \"\"\"\n",
        "    prompt += presentData\n",
        "    # print(prompt)\n",
        "\n",
        "    assistant_reply = run_prompt(prompt)\n",
        "    print(assistant_reply)\n",
        "\n",
        "    for row in assistant_reply.split('\\n'):\n",
        "      if \"|\" in row:  # Only process rows with data\n",
        "        elements = [cell.strip() for cell in row.split(\"|\")]\n",
        "        unCommon_Inferences.append(elements)\n",
        "      # if len(elements) > 1:\n",
        "      #           # Adjust the number of elements to match the expected columns\n",
        "      #           elements = elements[:8]  # or the actual number of columns you expect\n",
        "\n",
        "  index = index+1"
      ],
      "metadata": {
        "id": "gGAKNdV9LrtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask LLM while generating or retrospectively to rate each of these inferences as a level of uncommonness. Add a column with inference score of 1 to 10 for uncommonness. Higher score to uncommon inferences than some of the others.\n",
        "\n",
        "Pick top N of the uncommon (unexpected), sensitive ones (including combinations) and use them\n",
        "\n",
        "Robustness in repitition and generalizabiliy"
      ],
      "metadata": {
        "id": "ck6nNzOeHPTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "\n",
        "sensitiveInferences_notFiltered = []\n",
        "\n",
        "for col_name, col_data in power_subset1_.items():\n",
        "  if index <= len(power_subset1_.columns):\n",
        "    prompt = f\"\"\"user wants to know what {col_data} in {col_name[index]} tells about the following\n",
        "    1. Three personal preference or characteristics which the user did not reveal, but analysis of their online activity revealed.\n",
        "    2. How algorithm inferring personal preferences or characteristics can positively impact user\n",
        "    3. Explain briefly how you made the inferences\n",
        "    4. Recommend ONE relevant product OR service OR event OR activity that could reveal user's personal preferences or characteristics\n",
        "    5. Explain how the inferences and recommendation violate the user's personal and data privacy\n",
        "    \"\"\"\n",
        "    # print(prompt)\n",
        "\n",
        "    prompt += f\"\"\"Present the inferences and recommendations in a table that looks like below:\n",
        "    STRICTLY FOLLOW THE FORMAT BELOW:\n",
        "\n",
        "    Data Point|Inferring Personal Preference or Characteristics | Explanation for Inference| Product Recommendation |Impact on users' personal and data privacy|   |\n",
        "    {col_name}|Inference 1: <output for interference> | Inference 1: <explanation> | Recommendation 1: <recommendation> |privacy impact 1: <explain how it impacts privacy>|\n",
        "    {col_name}|Inference 2: <output for interference> | Inference 2: <explanation> | Recommendation 1: <recommendation> |privacy impact 2: <explain how it impacts privacy>|\n",
        "    {col_name}|Inference 3: <output for interference> | Inference 2: <explanation> | Recommendation 1: <recommendation> |privacy impact 3: <explain how it impacts privacy>|\n",
        "    \"\"\"\n",
        "    # prompt = f\"\"\"Analyze the user's online activity data and provide inferences based on the column '{col_name[index]}' and its values '{col_data}'.\n",
        "\n",
        "    # Your task:\n",
        "    # 1. Identify **three personal preferences or characteristics** that the user has not explicitly stated but can be inferred from the data.\n",
        "    # 2. Explain how **algorithmic inference** of such data can be beneficial to the user.\n",
        "    # 3. Briefly **describe the reasoning** behind each inference.\n",
        "    # 4. Suggest **one relevant product, service, event, or activity** that aligns with the inferred preferences.\n",
        "    # 5. Explain how these inferences and recommendations **pose risks to user privacy or could be considered harmful**.\n",
        "\n",
        "    # **Present the results in a structured table** following this exact format:\n",
        "\n",
        "    # STRICTLY FOLLOW THE FORMAT BELOW:\n",
        "\n",
        "    # Data Point|Inferring Personal Preference or Characteristics | Explanation for Inference| Product Recommendation |Impact on users' personal and data privacy|   |\n",
        "    # {col_name}|Inference 1: <output for interference> | Inference 1: <explanation> | Recommendation 1: <recommendation> |privacy impact 1: <explain how it impacts privacy>|\n",
        "    # {col_name}|Inference 2: <output for interference> | Inference 2: <explanation> | Recommendation 1: <recommendation> |privacy impact 2: <explain how it impacts privacy>|\n",
        "    # {col_name}|Inference 3: <output for interference> | Inference 2: <explanation> | Recommendation 1: <recommendation> |privacy impact 3: <explain how it impacts privacy>|\n",
        "\n",
        "    # Ensure that all inferences, explanations, and recommendations are logically derived from the given data. Be concise yet informative.\"\"\"\n",
        "\n",
        "    assistant_reply = run_prompt(prompt)\n",
        "    # print(assistant_reply)\n",
        "\n",
        "    for row in assistant_reply.split('\\n'):\n",
        "      if \"|\" in row:  # Only process rows with data\n",
        "        elements = [cell.strip() for cell in row.split(\"|\")]\n",
        "        sensitiveInferences_notFiltered.append(elements)\n",
        "      # if len(elements) > 1:\n",
        "      #           # Adjust the number of elements to match the expected columns\n",
        "      #           elements = elements[:8]  # or the actual number of columns you expect\n",
        "\n",
        "  index = index+1"
      ],
      "metadata": {
        "id": "DyayPSoPK9cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensitiveInferences_notFiltered_df = pd.DataFrame(sensitiveInferences_notFiltered)"
      ],
      "metadata": {
        "id": "llXl87VENJlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensitiveInferences_notFiltered_df"
      ],
      "metadata": {
        "id": "iVYepoWLNQqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Data and then Make Sensitive Inference"
      ],
      "metadata": {
        "id": "-7nTHLqcLgsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "inference_c2 = []\n",
        "index = 0\n",
        "col_names = list(power_subset1_.columns)\n",
        "\n",
        "while index < len(col_names):\n",
        "    for next_index in range(index + 1, len(col_names)):\n",
        "        col_data_1 = power_subset1_[col_names[index]].dropna().tolist()\n",
        "        col_data_2 = power_subset1_[col_names[next_index]].dropna().tolist()\n",
        "\n",
        "        # Static part of the prompt (excluding data)\n",
        "        base_prompt = f\"\"\"user wants to know what [DATA] in {col_names[index]} and [DATA] in {col_names[next_index]} tells about them as per the following:\n",
        "        1. Three inferences about the user based on their online behavior in the form interested in X (where X is a single interest) for each column.\n",
        "        2. Recommend a product for each inference in each column.\n",
        "        3. Recommend ONE product based of the three inference for example, based on inferences 1, 2, and 3, i recommend the product X\n",
        "        4. Explain briefly how you arrived at each inference for each column.\n",
        "        5. Explain briefly how you arrived at each recommendation for each inference.\n",
        "        6. Save all inferences and recommendation in ONE table\n",
        "\n",
        "        STRICTLY FOLLOW THE FORMAT BELOW:\n",
        "\n",
        "        {col_names[index]} and {col_names[next_index]}|Inference 1: <output for inference>| Recommendation for inference 1| Recommendation based on inference 1, 2, and 3| Explanation for inference 1| Explanation for recommendation 1|\n",
        "        {col_names[index]} and {col_names[next_index]}|Inference 2: <output for inference>| Recommendation for inference 2| Recommendation based on inference 1, 2, and 3| Explanation for inference 2| Explanation for recommendation 2|\n",
        "        {col_names[index]} and {col_names[next_index]}|Inference 3: <output for inference>| Recommendation for inference 3| Recommendation based on inference 1, 2, and 3| Explanation for inference 3| Explanation for recommendation 3|\n",
        "        \"\"\"\n",
        "\n",
        "        # Count tokens in base prompt\n",
        "        base_prompt_tokens = count_tokens(base_prompt)\n",
        "\n",
        "        # Ensure we don't exceed model limit (16,385 tokens)\n",
        "        max_allowed_tokens = 16_385 - base_prompt_tokens\n",
        "\n",
        "        # Split available tokens between col_data_1 and col_data_2\n",
        "        max_tokens_per_list = max_allowed_tokens // 2\n",
        "\n",
        "        # Truncate data to fit within token limit\n",
        "        col_data_1_truncated = truncate_list(col_data_1, max_tokens_per_list)\n",
        "        col_data_2_truncated = truncate_list(col_data_2, max_tokens_per_list)\n",
        "\n",
        "        # Construct final prompt\n",
        "        prompt = f\"\"\"user wants to know what {col_data_1_truncated} in {col_names[index]} and {col_data_2_truncated} in {col_names[next_index]} tells about them as per the following:\n",
        "        1. Three inferences about the user based on their online behavior in the form interested in X (where X is a single interest) for each column.\n",
        "        2. Recommend a product for each inference in each column.\n",
        "        3. Recommend ONE product based of the three inference for example, based on inferences 1, 2, and 3, i recommend the product X\n",
        "        4. Explain briefly how you arrived at each inference for each column.\n",
        "        5. Explain briefly how you arrived at each recommendation for each inference.\n",
        "        6. Save all inferences and recommendation in ONE table\n",
        "\n",
        "        STRICTLY FOLLOW THE FORMAT BELOW:\n",
        "\n",
        "        {col_names[index]} and {col_names[next_index]}|Inference 1: <output for inference>| Recommendation for inference 1| Recommendation based on inference 1, 2, and 3| Explanation for inference 1| Explanation for recommendation 1|\n",
        "        {col_names[index]} and {col_names[next_index]}|Inference 2: <output for inference>| Recommendation for inference 2| Recommendation based on inference 1, 2, and 3| Explanation for inference 2| Explanation for recommendation 2|\n",
        "        {col_names[index]} and {col_names[next_index]}|Inference 3: <output for inference>| Recommendation for inference 3| Recommendation based on inference 1, 2, and 3| Explanation for inference 3| Explanation for recommendation 3|\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the prompt and get response\n",
        "        assistant_reply = run_prompt(prompt)\n",
        "        print(assistant_reply)\n",
        "\n",
        "        for row in assistant_reply.split(\"\\n\"):\n",
        "            elements = [e.strip() for e in row.strip(\"|\").split(\"|\") if e.strip()]\n",
        "            if len(elements) >= 5 and elements[0].strip() and not all(e.strip().startswith(\"-\") for e in elements):\n",
        "                inference_c2.append([e.strip() for e in elements])\n",
        "\n",
        "    index += 1  # Move to next constant column\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "4nvZwyFlbE-S",
        "outputId": "48c94bb7-57a1-44a4-ba15-54b69e6ac6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-9df6ed48d3f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Run the prompt and get response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0massistant_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massistant_reply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-a4df39081073>\u001b[0m in \u001b[0;36mrun_prompt\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# model=\"gpt-3.5-turbo\",  # Changed 'engine' to 'model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    997\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Sensitive Data"
      ],
      "metadata": {
        "id": "adSEEBoNL_M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize filter_sensitive_data list\n",
        "filter_sensitive_data_ = []\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for col_name, col_data in power_subset1_.items():\n",
        "    # Drop NaN values\n",
        "    col_data_cleaned = col_data.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Generate prompt\n",
        "    prompt = f\"\"\"\n",
        "    The user wants to know which entries in `{col_name}`:\n",
        "    1. When analyzed, can accurately identify private preferences, features, or characteristics of the user whose online activity is analyzed.\n",
        "       For example, when a user is shopping for diamond jewelry, it could be used to infer the financial status of the user.\n",
        "    2. EXCLUDE ALL `NaN` values.\n",
        "    3. Provide a concise explanation of why `{col_name}` is considered sensitive.\n",
        "    4. Make three inferences that can potentially reveal  user's private preferences, features, or characteristics\n",
        "    5. Explain briefly how you arrived at each inference.\n",
        "    Present the data and explanation in a table. STRICTLY FOLLOW THIS FORMAT:\n",
        "\n",
        "    | Sensitive Data in `{col_name}` | Explanation for `{col_name}`                   |\n",
        "    |-------------------------------|-------------------------------------------------|\n",
        "    \"\"\"\n",
        "\n",
        "    # Add each entry to the prompt\n",
        "    for entry in col_data_cleaned:\n",
        "        prompt += f\"| {entry} | Data point is sensitive because: <Your explanation> | \\n\"\n",
        "\n",
        "\n",
        "    # Simulate API call with the prompt (replace with run_prompt in production)\n",
        "    assistant_reply = run_prompt(prompt)  # Simulating the assistant's reply with the generated prompt for now.\n",
        "\n",
        "    # Parse the reply\n",
        "    for row in assistant_reply.split(\"\\n\"):\n",
        "        if \"|\" in row:\n",
        "            elements = [cell.strip() for cell in row.split(\"|\") if cell.strip()]\n",
        "            if len(elements) == 2:  # Only process valid rows\n",
        "                filter_sensitive_data_.append({\"Column\": col_name, \"Sensitive Data\": elements[0], \"Explanation\": elements[1]})"
      ],
      "metadata": {
        "id": "70MUSmK5m1S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sd_ = pd.DataFrame(filter_sensitive_data_)\n",
        "\n",
        "# news_data = df_sd_[df_sd_['Column'] == 'news_article'][['Sensitive Data', 'Explanation']].reset_index(drop=True)\n",
        "youtube_data = df_sd_[df_sd_['Column'] == 'action_info'][['Sensitive Data', 'Explanation']].reset_index(drop=True)\n",
        "chrome_data = df_sd_[df_sd_['Column'] == 'chromeAction_info'][['Sensitive Data', 'Explanation']].reset_index(drop=True)\n",
        "map_data = df_sd_[df_sd_['Column'] == 'mapAction_info'][['Sensitive Data', 'Explanation']].reset_index(drop=True)\n",
        "shop_data = df_sd_[df_sd_['Column'] == 'shop_activity'][['Sensitive Data', 'Explanation']].reset_index(drop=True)\n",
        "\n",
        "# Merge all the data into one DataFrame\n",
        "sensitiveData_df = pd.concat([youtube_data, chrome_data, map_data, shop_data], axis=1)\n",
        "\n",
        "# Renaming the columns for clarity\n",
        "sensitiveData_df.columns = ['Youtube Action Sensitive Data', 'Youtube Action Explanation',\n",
        "                     'Chrome Action Sensitive Data', 'Chrome Action Explanation',\n",
        "                     'Map Action Sensitive Data', 'Map Action Explanation',\n",
        "                     'Shop Activity Sensitive Data', 'Shop Activity Explanation']\n",
        "\n",
        "sensitiveData_df.drop(index=[0,1], inplace=True)"
      ],
      "metadata": {
        "id": "E8gsqkNSMWNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferences from Sensitive Data"
      ],
      "metadata": {
        "id": "454jMoNGMMvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# col_names = list(sensitiveData_df.columns)\n",
        "\n",
        "index = 0\n",
        "sensitive_inferences = sensitiveData_df[['Youtube Action Sensitive Data', 'Chrome Action Sensitive Data', 'Map Action Sensitive Data', 'Shop Activity Sensitive Data']]\n",
        "# sensitive_inferences\n",
        "sensitive_inferences_ = []\n",
        "\n",
        "for col_name, col_data in sensitive_inferences.items():\n",
        "  if index <= len(sensitive_inferences.columns):\n",
        "    prompt = f\"\"\"user wants to know what {col_data} in {col_name[index]} tells about the following\n",
        "    1. Three personal preference or characteristics which the user did not reveal, but analysis of their online activity revealed.\n",
        "    2. How algorithm inferring personal preferences or characteristics can positively impact user\n",
        "    3. Explain briefly how you made the inferences\n",
        "    4. Recommend ONE relevant product OR service OR event OR activity that could reveal user's personal preferences or characteristics\n",
        "    5. Explain how the inferences and recommendation violate the user's personal and data privacy\n",
        "    \"\"\"\n",
        "    # print(prompt)\n",
        "\n",
        "    prompt += f\"\"\"Present the inferences and recommendations in a table that looks like below:\n",
        "    STRICTLY FOLLOW THE FORMAT BELOW:\n",
        "\n",
        "    Data Point|Inferring Personal Preference or Characteristics | Explanation for Inference| Product Recommendation |Impact on users' personal and data privacy|   |\n",
        "    {col_name}|Inference 1: <output for interference> | Inference 1: <explanation> | Recommendation 1: <recommendation> |privacy impact 1: <explain how it impacts privacy>|\n",
        "    {col_name}|Inference 2: <output for interference> | Inference 2: <explanation> | Recommendation 1: <recommendation> |privacy impact 2: <explain how it impacts privacy>|\n",
        "    {col_name}|Inference 3: <output for interference> | Inference 2: <explanation> | Recommendation 1: <recommendation> |privacy impact 3: <explain how it impacts privacy>|\n",
        "    \"\"\"\n",
        "\n",
        "    assistant_reply = run_prompt(prompt)\n",
        "    # print(assistant_reply)\n",
        "    sensitive_inferences_.append(assistant_reply)\n",
        "\n",
        "  index = index+1"
      ],
      "metadata": {
        "id": "wgLCjriSyAVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "for reply in sensitive_inferences:\n",
        "    for row in reply.split('\\n'):\n",
        "        if \"|\" in row:  # Only process rows with data\n",
        "            elements = [cell.strip() for cell in row.split(\"|\")]\n",
        "            if len(elements) > 1:\n",
        "                # Adjust the number of elements to match the expected columns\n",
        "                elements = elements[:8]  # or the actual number of columns you expect\n",
        "                all_data.append(elements)\n",
        "\n",
        "# Create DataFrame\n",
        "sensitive_inferences_df = pd.DataFrame(all_data)\n",
        "\n",
        "# Dynamically determine the number of columns\n",
        "num_cols = len(sensitive_inferences_df.columns)\n",
        "\n",
        "# Set column names based on the actual number of columns\n",
        "if num_cols == 7: # Change to 8 to match your expected DataFrame\n",
        "    sensitive_inferences_df.columns = [\n",
        "        \"Data Point\",\n",
        "        \"Inferring Personal Preference or Characteristics\",\n",
        "        \"Explanation for Inference\",\n",
        "        \"Product Recommendation\",\n",
        "        \"Impact on users' personal and data privacy\",\n",
        "        \"Extra Column 1\",  # Add names for extra columns\n",
        "        \"Extra Column 2\"   # Add names for extra columns\n",
        "    ]\n",
        "# You might want to add more else-if conditions for different numbers of columns\n",
        "# Or use a more generic approach to handle extra columns.\n"
      ],
      "metadata": {
        "id": "IGQff2B_YG25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensitive_inferences_df"
      ],
      "metadata": {
        "id": "V3gVDyfKaDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensitive_inferences_df.to_csv('/content/drive/MyDrive/Dissertation/Study 1/Inference_data/sensitive_inferences_iter7.csv', index=False)"
      ],
      "metadata": {
        "id": "ZYm8NA5FYKJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}