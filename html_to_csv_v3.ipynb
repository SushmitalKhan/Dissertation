{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushmitalKhan/Dissertation/blob/main/html_to_csv_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNInHTcKikIs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "import openai\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfuG4-8XiZF8",
        "outputId": "edaa72b7-8cb8-4ece-e136-cc4d4df5d242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls9McgWfan90"
      },
      "outputs": [],
      "source": [
        "input_html = '/content/drive/MyDrive/Dissertation/Study 1/googleData/html/Chrome/takeout1_chrome_MyActivity.html'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozs-owNVpn_B"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_and_remove_datetime(text):\n",
        "    # Define a regex pattern for the specified date and time format\n",
        "    datetime_pattern = r\"(?P<month>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))\\s+(?P<day>\\d{1,2}),\\s+(?P<year>\\d{4}),\\s+(?P<hour>\\d{1,2}):(?P<minute>\\d{2}):(?P<second>\\d{2})\\s+(?P<meridian>[APap][Mm])\\s+(?P<timezone>[A-Z]+)\"\n",
        "\n",
        "    # Check if the pattern is found in the text\n",
        "    match = re.search(datetime_pattern, text)\n",
        "\n",
        "    if match:\n",
        "        # Extract date and time components using named groups\n",
        "        date = match.group('month') + \" \" + match.group('day') + \", \" + match.group('year')\n",
        "        time = match.group('hour') + \":\" + match.group('minute') + \":\" + match.group('second') + \" \" + match.group('meridian') + \" \" + match.group('timezone')\n",
        "\n",
        "        # Remove the extracted date and time from the original text\n",
        "        # cleaned_text = re.sub(datetime_pattern, \"\", text)\n",
        "\n",
        "        return date, time\n",
        "    else:\n",
        "        return None, None, text  # Return original text if not found"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_html_file_fast(input_html):\n",
        "    \"\"\"Efficiently reads an HTML file and returns a parsed BeautifulSoup object.\"\"\"\n",
        "    with open(input_html, 'r', encoding='utf-8') as f:\n",
        "        return BeautifulSoup(f.read(), 'lxml')  # Use 'lxml' for speed\n",
        "\n",
        "soup = read_html_file_fast(input_html)"
      ],
      "metadata": {
        "id": "NoHEaiop2nZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSB9WrX6azFy"
      },
      "outputs": [],
      "source": [
        "output_filename = os.path.splitext(os.path.basename(input_html))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59GqPValdB7G"
      },
      "outputs": [],
      "source": [
        "output_filepath = os.path.join(os.path.dirname('/content/drive/MyDrive/Dissertation/Study 1/googleData/CSV_data'), f\"{output_filename}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ELavZM0Msyp"
      },
      "outputs": [],
      "source": [
        "# Extract all activity entries\n",
        "entries = soup.find_all('div', class_='outer-cell')\n",
        "\n",
        "data = []\n",
        "for entry in entries:\n",
        "    # Extract source (Column 1)\n",
        "    source_tag = entry.find('p', class_='mdl-typography--title')\n",
        "    source = source_tag.get_text(strip=True) if source_tag else None\n",
        "\n",
        "    # Extract link (Column 2)\n",
        "    link_tag = entry.find('a')\n",
        "    link = None\n",
        "    if link_tag:\n",
        "        match = re.search(r'q=(https://[^\\&]+)', link_tag['href'])\n",
        "        link = match.group(1) if match else link_tag['href']\n",
        "\n",
        "    # Extract title (Column 3)\n",
        "    title = None if \"https://\" in link_tag.get_text(strip=True) else link_tag.get_text(strip=True)\n",
        "\n",
        "    # Extract date and time (Column 4 & 5)\n",
        "    text_content = entry.find('div', class_='content-cell').get_text(strip=True)\n",
        "    # date_time_match = re.search(r'(\\w+ \\d{1,2}, \\d{4}), (\\d{1,2}:\\d{2}:\\d{2} [APM]+ [A-Z]+)', text_content)\n",
        "    date_time_match = extract_and_remove_datetime(text_content)\n",
        "\n",
        "    # Access the elements of the tuple returned by extract_and_remove_datetime\n",
        "    date = date_time_match[0] if date_time_match[0] is not None else None # Access the first element for date\n",
        "    time = date_time_match[1] if date_time_match[1] is not None else None # Access the second element for time\n",
        "\n",
        "    # Append data to list\n",
        "    data.append([source, link, title, date, time])\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data, columns=['Search Site', 'Search Link', 'Search Title', 'Search Date', 'Search Time'])\n",
        "\n",
        "# Save to CSV\n",
        "# df.to_csv(output_filepath, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wwARAi-Ho1WmC4jOyS87u4gznajESUKP",
      "authorship_tag": "ABX9TyMKfQ68McW3spcJ1hdQGV0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}