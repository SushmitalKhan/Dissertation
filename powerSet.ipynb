{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fxWQhRnaV7Sy7cjlXTMRU5EAPQzInioi",
      "authorship_tag": "ABX9TyNgSSO+dzjV1T93Mvfn/M5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushmitalKhan/Dissertation/blob/main/powerSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "import os\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "LNInHTcKikIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfuG4-8XiZF8",
        "outputId": "e990dc43-194a-43e0-9974-f8d74e46deec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `READ ALL FILES`\n",
        "\n"
      ],
      "metadata": {
        "id": "NwyF7vJ_zVeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing CSV files\n",
        "folder_path = \"/content/drive/MyDrive/Dissertation/Study 1/googleData/CSV_data\"  # Change this to your folder path\n",
        "\n",
        "# Get a list of all CSV files in the folder\n",
        "csv_files = glob(os.path.join(folder_path, \"*.csv\"))  # Find all CSV files\n",
        "\n",
        "df_names = []  # Store DataFrame names\n",
        "search_title_dfs = []  # List to store \"Search Title\" columns"
      ],
      "metadata": {
        "id": "Nq26ZJ_lzTWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `MAKING POWERSET WHERE ALL THE SEARCH TITLES FOR ALL ACTIVITY AND YEAR IS IN ONE BIG CSV`"
      ],
      "metadata": {
        "id": "QSqS4274zgee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for file in csv_files:\n",
        "    output_filename = os.path.splitext(os.path.basename(file))[0]  # Extract filename without extension\n",
        "    globals()[output_filename] = pd.read_csv(file)  # Create a variable with the filename\n",
        "\n",
        "    # Get columns that contain \"Search Title\" and \"Search Date\"\n",
        "    search_title_cols = [col for col in globals()[output_filename].columns if \"Search Title\" in col]\n",
        "    search_date_cols = [col for col in globals()[output_filename].columns if \"Search Date\" in col]\n",
        "\n",
        "    if search_title_cols and search_date_cols:  # Check if there are matching columns\n",
        "        # Extract relevant columns\n",
        "        search_title_df = globals()[output_filename][search_title_cols + search_date_cols]\n",
        "\n",
        "        # Convert \"Search Date\" columns to datetime and extract year\n",
        "        for date_col in search_date_cols:\n",
        "            search_title_df[date_col] = pd.to_datetime(search_title_df[date_col], errors='coerce').dt.year\n",
        "\n",
        "        # Lowercase and remove extra spaces or characters\n",
        "        for col in search_title_cols:\n",
        "            search_title_df[col] = search_title_df[col].str.lower().str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "        # Drop NaN values after the transformations\n",
        "        search_title_df.dropna(subset=search_title_cols, inplace=True)\n",
        "\n",
        "        # Drop duplicates\n",
        "        search_title_df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # Rename columns by prefixing the filename\n",
        "        search_title_df.columns = [f\"{output_filename}_{col}\" for col in search_title_df.columns]\n",
        "\n",
        "        # Append to the list of dataframes\n",
        "        search_title_dfs.append(search_title_df)\n",
        "\n",
        "    df_names.append(output_filename)  # Store DataFrame name\n",
        "\n",
        "# Combine all extracted \"Search Title\" and \"Search Date\" columns into one DataFrame\n",
        "if search_title_dfs:\n",
        "    combined_search_titles_df = pd.concat(search_title_dfs, axis=1)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Loaded {len(csv_files)} CSV files. Extracted 'Search Title' and 'Search Date' columns into a new DataFrame.\")\n",
        "print(\"DataFrames created:\", df_names)  # Print names of all DataFrames\n",
        "\n",
        "# Display the first few rows of the combined DataFrame\n",
        "# print(combined_search_titles_df.head())  # Optionally display combined DataFrame\n"
      ],
      "metadata": {
        "id": "0dSRJ_B1CRsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"/content/drive/MyDrive/Dissertation/Study 1/Powerset/powerset_all_year_col.csv\"\n",
        "\n",
        "combined_search_titles_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqB2hXWErs-5",
        "outputId": "d8479530-9a8a-4553-ca7c-86b15e6fbc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/drive/MyDrive/Dissertation/Study 1/Powerset/powerset_all_year_col.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for file in csv_files:\n",
        "    output_filename = os.path.splitext(os.path.basename(file))[0]  # Extract filename without extension\n",
        "    globals()[output_filename] = pd.read_csv(file)  # Create a variable with the filename\n",
        "\n",
        "    # Get columns that contain \"Search Title\" and \"Search Date\"\n",
        "    search_title_cols = [col for col in globals()[output_filename].columns if \"Search Title\" in col]\n",
        "    search_date_cols = [col for col in globals()[output_filename].columns if \"Search Date\" in col]\n",
        "\n",
        "    if search_title_cols and search_date_cols:  # Check if there are matching columns\n",
        "        # Extract relevant columns\n",
        "        search_title_df = globals()[output_filename][search_title_cols + search_date_cols]\n",
        "\n",
        "        # Convert \"Search Date\" columns to datetime and extract year\n",
        "        for date_col in search_date_cols:\n",
        "            search_title_df[date_col] = pd.to_datetime(search_title_df[date_col], errors='coerce').dt.year\n",
        "\n",
        "        # Lowercase and remove extra spaces or characters\n",
        "        for col in search_title_cols:\n",
        "            search_title_df[col] = search_title_df[col].str.lower().str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "        # Drop NaN values after the transformations\n",
        "        search_title_df.dropna(subset=search_title_cols, inplace=True)\n",
        "\n",
        "        # Drop duplicates\n",
        "        search_title_df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # Rename columns by prefixing the filename\n",
        "        search_title_df.columns = [f\"{output_filename}_{col}\" for col in search_title_df.columns]\n",
        "\n",
        "        # Append to the list of dataframes\n",
        "        search_title_dfs.append(search_title_df)\n",
        "\n",
        "    df_names.append(output_filename)  # Store DataFrame name\n",
        "\n",
        "# Combine all extracted \"Search Title\" and \"Search Date\" columns into one DataFrame\n",
        "if search_title_dfs:\n",
        "    combined_search_titles_df = pd.concat(search_title_dfs, axis=1)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Loaded {len(csv_files)} CSV files. Extracted 'Search Title' and 'Search Date' columns into a new DataFrame.\")\n",
        "print(\"DataFrames created:\", df_names)  # Print names of all DataFrames\n",
        "\n",
        "# Display the first few rows of the combined DataFrame\n",
        "# print(combined_search_titles_df.head())  # Optionally display combined DataFrame\n"
      ],
      "metadata": {
        "id": "w-cMZxwQxgxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_search_titles_df"
      ],
      "metadata": {
        "id": "Tkd9W9AXxfn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `DROPPING COLUMNS THAT HAS LESS THAN 200 DATA POINTS`"
      ],
      "metadata": {
        "id": "BOdCdhlLz7QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the 'Search Date' columns (that are now years) exist in the DataFrame\n",
        "for date_col in combined_search_titles_df.columns:\n",
        "    if \"Search Date\" in date_col:\n",
        "        # Filter data by year (since it's in the 'Search Date' column)\n",
        "        for year in combined_search_titles_df[date_col].unique():\n",
        "            # Filter the DataFrame by the current year\n",
        "            year_filtered_df = combined_search_titles_df[combined_search_titles_df[date_col] == year]\n",
        "\n",
        "            # Save to a CSV file, naming it based on the year\n",
        "            # output_filename = f\"/content/drive/MyDrive/Dissertation/Study 1/Powerset/search_titles_{year}.csv\"\n",
        "            year_filtered_df.to_csv(output_filename, index=False)\n",
        "            print(f\"Saved data for year {year} to {output_filename}\")"
      ],
      "metadata": {
        "id": "4OX8xQsGEM6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of non-NaN values for each column\n",
        "column_lengths = combined_search_titles_df.count()\n",
        "\n",
        "# Filter columns that have at least 200 non-NaN values\n",
        "columns_to_keep = column_lengths[column_lengths >= 200].index\n",
        "\n",
        "# Drop columns that don't meet the criteria\n",
        "combined_search_titles_df = combined_search_titles_df[columns_to_keep]\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(combined_search_titles_df.head())"
      ],
      "metadata": {
        "id": "JI3w0P_KnyM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"/content/drive/MyDrive/Dissertation/Study 1/Powerset/powerset_col200++.csv\"\n",
        "\n",
        "combined_search_titles_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsNcqHcZDzIZ",
        "outputId": "04e1cfe1-1532-424e-c987-e78f7a3e09bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/drive/MyDrive/Dissertation/Study 1/Powerset/powerset_col200++.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `FILTERING SEARCH TITLES FOR GIVEN ACTIVITY BY YEAR, AND SAVING SEARCH TITLES FILTERED BY YEARS AS SEPARATE CSV`"
      ],
      "metadata": {
        "id": "7AfqmEq50B0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing CSV files\n",
        "# folder_path = \"/content/drive/MyDrive/Dissertation/Study 1/googleData/CSV_data\"  # Change this to your folder path\n",
        "\n",
        "# Get a list of all CSV files in the folder\n",
        "# csv_files = glob(os.path.join(folder_path, \"*.csv\"))  # Find all CSV files\n",
        "\n",
        "# df_names = []\n",
        "\n",
        "# Read each CSV file and assign it to a variable with the same name as the file\n",
        "for file in csv_files:\n",
        "    output_filename = os.path.splitext(os.path.basename(file))[0]  # Extract filename without extension\n",
        "    globals()[output_filename] = pd.read_csv(file)  # Create a variable with the filename\n",
        "\n",
        "    # Drop duplicates from the \"Search Title\" column\n",
        "    if \"Search Title\" in globals()[output_filename].columns:\n",
        "        globals()[output_filename].drop_duplicates(subset=[\"Search Title\"], inplace=True)\n",
        "        globals()[output_filename].dropna(subset=[\"Search Title\"], inplace=True)\n",
        "\n",
        "    # Rename columns by prefixing with filename\n",
        "    globals()[output_filename].columns = [f\"{output_filename}_{col}\" for col in globals()[output_filename].columns]\n",
        "\n",
        "    df_names.append(output_filename)  # Store DataFrame name\n",
        "\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Loaded {len(csv_files)} CSV files as separate DataFrames.\")\n",
        "print(\"DataFrames created:\", df_names)  # Print names of all DataFrames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFvhfsEOQXtC",
        "outputId": "2784b251-08b5-4e7f-b742-43f8202dec4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 CSV files as separate DataFrames.\n",
            "DataFrames created: ['takeout1_videoSearch_MyActivity', 'takeout1_imageSearch_MyActivity', 'takeout1_chrome_MyActivity', 'takeout1_gNews_MyActivity', 'takeout1_books_MyActivity', 'takeout1_playstore_MyActivity', 'takeout1_maps_MyActivity', 'takeout1_YT_search-history', 'takeout1_YT_watch-history', 'takeout1_playGames_MyActivity']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output folder\n",
        "output_folder = \"/content/drive/MyDrive/Dissertation/Study 1/Filtered by Year\"\n",
        "os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
        "\n",
        "def process_dataframe(df, df_name):\n",
        "    \"\"\"\n",
        "    Processes a given DataFrame:\n",
        "    - Extracts 'Search Title' and 'Search Date' columns\n",
        "    - Filters data by year\n",
        "    - Saves each year's 'Search Title' in separate CSV files\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to process\n",
        "    - df_name: Name of the original DataFrame (for file naming)\n",
        "    \"\"\"\n",
        "    # Identify relevant columns\n",
        "    search_title_cols = [col for col in df.columns if \"Search Title\" in col]\n",
        "    search_date_cols = [col for col in df.columns if \"Search Date\" in col or \"Date\" in col]\n",
        "\n",
        "    if not search_title_cols or not search_date_cols:\n",
        "        print(f\"Skipping {df_name}: No valid 'Search Title' or 'Search Date' column found.\")\n",
        "        return\n",
        "\n",
        "    # Extract necessary columns\n",
        "    search_data = df[search_title_cols + search_date_cols].copy()\n",
        "\n",
        "    # Convert date column to datetime and extract year\n",
        "    date_col = search_date_cols[0]  # Assume first matching column is correct\n",
        "    search_data[date_col] = pd.to_datetime(search_data[date_col], errors='coerce').dt.year\n",
        "\n",
        "    # Drop NaN values in Search Title\n",
        "    search_data.dropna(subset=search_title_cols, inplace=True)\n",
        "\n",
        "    # Remove duplicates\n",
        "    search_data.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Get unique years\n",
        "    unique_years = search_data[date_col].dropna().unique()\n",
        "\n",
        "    for year in unique_years:\n",
        "        # Filter data for the specific year\n",
        "        year_data = search_data[search_data[date_col] == year][search_title_cols]\n",
        "\n",
        "        # Save to CSV\n",
        "        output_filename = f\"{df_name}_search_titles_{year}.csv\"\n",
        "        output_path = os.path.join(output_folder, output_filename)\n",
        "        year_data.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"Saved: {output_filename} ({len(year_data)} rows)\")\n",
        "\n",
        "# List of DataFrame names\n",
        "df_names = [\n",
        "    \"takeout1_videoSearch_MyActivity\",\n",
        "    \"takeout1_imageSearch_MyActivity\",\n",
        "    \"takeout1_chrome_MyActivity\",\n",
        "    \"takeout1_gNews_MyActivity\",\n",
        "    \"takeout1_books_MyActivity\",\n",
        "    \"takeout1_playstore_MyActivity\",\n",
        "    \"takeout1_maps_MyActivity\",\n",
        "    \"takeout1_YT_search-history\",\n",
        "    \"takeout1_YT_watch-history\",\n",
        "    \"takeout1_playGames_MyActivity\"\n",
        "]\n",
        "\n",
        "# Process each DataFrame\n",
        "for df_name in df_names:\n",
        "    if df_name in globals():\n",
        "        process_dataframe(globals()[df_name], df_name)\n",
        "    else:\n",
        "        print(f\"Skipping {df_name}: DataFrame not found in memory.\")\n"
      ],
      "metadata": {
        "id": "TkzIM1edV_Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Folder path containing the CSV files\n",
        "folder_path = \"/content/drive/MyDrive/Dissertation/Study 1/Filtered by Year/2016\"  # Replace with the actual folder path\n",
        "\n",
        "# Get a list of all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
        "\n",
        "# Read and concatenate all CSV files into one DataFrame\n",
        "df_combined = pd.concat((pd.read_csv(file) for file in csv_files), ignore_index=True)\n",
        "\n",
        "# Sort each column so that non-NaN values appear first\n",
        "df_sorted = df_combined.apply(lambda col: col.dropna().tolist() + [None] * col.isna().sum(), axis=0)\n",
        "\n",
        "\n",
        "# Save the combined DataFrame (optional)\n",
        "df_sorted.to_csv(\"/content/drive/MyDrive/Dissertation/Study 1/Powerset/powerset_by_year/allActivity_2016.csv\", index=False)\n",
        "\n",
        "# Display the first few rows\n",
        "# print(df_combined.head())\n"
      ],
      "metadata": {
        "id": "jEkHe_VPbGv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted"
      ],
      "metadata": {
        "id": "WuJodgo4iEP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_nan_counts = df_combined.count()"
      ],
      "metadata": {
        "id": "Y-10Aoi0byd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}